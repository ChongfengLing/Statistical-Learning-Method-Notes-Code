{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded\n",
      "Datasets loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.datasets import load_iris\n",
    "import treePlotter # local\n",
    "\n",
    "print('Packages loaded')\n",
    "\n",
    "iris=load_iris() # dataset for classification\n",
    "boston=load_boston() # dataset for regression\n",
    "\n",
    "iris\n",
    "boston\n",
    "\n",
    "print('Datasets loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def CARTClassification(self):\n",
    "        pass\n",
    "\n",
    "    def CARTRegression(self):\n",
    "        pass\n",
    "\n",
    "    def ID3_create(self, train_set, features, labels, tol=0.1, visible=False):\n",
    "        \"\"\"\n",
    "        create ID3 tree\n",
    "        :param train_set: m*n ndarray. m: samples, n: features\n",
    "        :param features: n size vector\n",
    "        :param labels: m size ndarray\n",
    "        :param tol: tolerate for pre-pruning\n",
    "        :return: ID3 tree in dict type\n",
    "        \"\"\"\n",
    "        '''\n",
    "        three conditions to stop iteration:\n",
    "          1. all labelss are the same\n",
    "          2. no feature\n",
    "          3. info_gain < tol. pre-pruning\n",
    "          4. same training values but different labels\n",
    "        '''\n",
    "        train_set = np.array(train_set)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        if len(np.unique(labels)) == 1:  # condition 1\n",
    "            return labels[0]\n",
    "\n",
    "        if train_set.shape[1] == 0:  # condition 2\n",
    "            return np.sort(labels)[-1]  # return the most frequency value\n",
    "\n",
    "        # condition 3 & 4\n",
    "        # not finished\n",
    "\n",
    "\n",
    "        # get best feature\n",
    "        best_feature_index, best_info_gain = self.ID3_best_feature(train_set, labels)\n",
    "        best_feature_name = features[best_feature_index]\n",
    "        print('best selected feature is ', best_feature_name, ', its information gain is ', best_info_gain)\n",
    "\n",
    "        ID3Tree = {best_feature_name: {}}  # return feature name as a dict key\n",
    "        # return unique values under the feature and as the node(key)\n",
    "        tree_nodes = np.unique(train_set.T[best_feature_index])\n",
    "        # small feature set for dealing feature set depending on its index\n",
    "        features = np.delete(features, best_feature_index)\n",
    "        # iteration in these nodes\n",
    "        for node in tree_nodes:\n",
    "            train_sample_index = train_set.T[best_feature_index] == node\n",
    "            node_labels = labels[train_sample_index]\n",
    "            # small train set with node feature's column equal node value\n",
    "            node_train_set = self.spilt_dataset(train_set, best_feature_index, node)\n",
    "            # small train set without node feature\n",
    "            node_train_set = np.delete(node_train_set, best_feature_index, axis=1)\n",
    "            # iteration\n",
    "            ID3Tree[best_feature_name][node] = self.ID3_create(node_train_set, features, node_labels)\n",
    "\n",
    "        if visible is True:\n",
    "            treePlotter.ID3_Tree(tree)\n",
    "        return ID3Tree\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(array):\n",
    "        \"\"\"\n",
    "        calculate bit entropy\n",
    "        :param array: 1-D numpy array\n",
    "        :return: entropy in bit\n",
    "        \"\"\"\n",
    "        count_array = np.unique(array, return_counts=True)[1]  # unique values and its occurrences\n",
    "        probability = count_array / array.size  # probability of values\n",
    "        h_p = np.dot(-probability, np.log2(probability))  # entropy\n",
    "        return h_p\n",
    "\n",
    "    @staticmethod\n",
    "    def conditional_entropy(Y, X):\n",
    "        \"\"\"\n",
    "        get conditional entropy H(Y|X)\n",
    "        :param Y: random variable, 1-D numpy array\n",
    "        :param X: given random variable, 1-D numpy array\n",
    "        :return: conditional entropy of Y given X, H(Y|X)\n",
    "        \"\"\"\n",
    "        Y = np.array(Y)\n",
    "        X = np.array(X)\n",
    "        hY_X = 0  # initialization\n",
    "        X_value, X_count = np.unique(X, return_counts=True)  # unique values and its occurrences\n",
    "        for xi in X_value:\n",
    "            index = np.argwhere(X == xi)  # get index of X=xi\n",
    "            p_xi = index.size / X.size  # P(X=xi)\n",
    "            Yi = Y[index]  # get yi given xi\n",
    "            hYi_xi = DecisionTree.entropy(np.array(Yi))  # H(Y|X=xi)\n",
    "            hY_X += p_xi * hYi_xi\n",
    "        return hY_X\n",
    "\n",
    "    @staticmethod\n",
    "    def info_gain(Y, X):\n",
    "        \"\"\"\n",
    "        get information gain G(Y,X)\n",
    "        :param Y: random variable, 1-D numpy array\n",
    "        :param X: given random variable, 1-D numpy array\n",
    "        :return: information gain of Y given X, G(Y|X)\n",
    "        \"\"\"\n",
    "        return DecisionTree.entropy(Y) - DecisionTree.conditional_entropy(Y, X)\n",
    "\n",
    "    @staticmethod\n",
    "    def spilt_dataset(dataset, colume, value):\n",
    "        \"\"\"\n",
    "        dataset with small samples\n",
    "        :param dataset: m*n ndarray\n",
    "        :param colume:  axis\n",
    "        :param value: compared value\n",
    "        :return: l*n ndarray, l<m\n",
    "        \"\"\"\n",
    "        dataset = pd.DataFrame(dataset)\n",
    "        df = dataset[dataset[colume] == value]\n",
    "\n",
    "        return np.array(df)\n",
    "\n",
    "    @staticmethod\n",
    "    def ID3_best_feature(train_set, labels):\n",
    "        \"\"\"\n",
    "        return the feature with the highest infomation gain\n",
    "        :param train_set: m*n ndarray. m: samples, n: features\n",
    "        :param labels: m size ndarray.\n",
    "        :return: best feature index and its infomation gain\n",
    "        \"\"\"\n",
    "        features = train_set.shape[1]  # number of features\n",
    "        tmp = np.ones(features) * -1  # store info gain\n",
    "        for i in range(features):  # calculate info gain of each features\n",
    "            feature_list = train_set.T[i]\n",
    "            gain = DecisionTree.info_gain(labels, feature_list)\n",
    "            tmp[i] = gain\n",
    "            print(\"the info gain of %d th feature in ID3 is: %.3f\" % (i, gain))\n",
    "        best_feature = np.argmax(tmp)\n",
    "        best_info_gain = tmp[best_feature]\n",
    "        return best_feature, best_info_gain"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded\n",
      "the info gain of 0 th feature in ID3 is: 0.877\n",
      "the info gain of 1 th feature in ID3 is: 0.517\n",
      "the info gain of 2 th feature in ID3 is: 1.446\n",
      "the info gain of 3 th feature in ID3 is: 1.436\n",
      "best selected feature is  petal length (cm) , its information gain is  1.4463165236458\n",
      "the info gain of 0 th feature in ID3 is: 0.544\n",
      "the info gain of 1 th feature in ID3 is: 0.544\n",
      "the info gain of 2 th feature in ID3 is: 0.544\n",
      "best selected feature is  sepal length (cm) , its information gain is  0.5435644431995964\n",
      "the info gain of 0 th feature in ID3 is: 1.000\n",
      "the info gain of 1 th feature in ID3 is: 0.500\n",
      "the info gain of 2 th feature in ID3 is: 0.311\n",
      "best selected feature is  sepal length (cm) , its information gain is  1.0\n",
      "the info gain of 0 th feature in ID3 is: 0.571\n",
      "the info gain of 1 th feature in ID3 is: 0.971\n",
      "the info gain of 2 th feature in ID3 is: 0.971\n",
      "best selected feature is  sepal width (cm) , its information gain is  0.9709505944546686\n",
      "the info gain of 0 th feature in ID3 is: 0.811\n",
      "the info gain of 1 th feature in ID3 is: 0.811\n",
      "the info gain of 2 th feature in ID3 is: 0.811\n",
      "best selected feature is  sepal length (cm) , its information gain is  0.8112781244591328\n",
      "the info gain of 0 th feature in ID3 is: 0.544\n",
      "the info gain of 1 th feature in ID3 is: 0.199\n",
      "the info gain of 2 th feature in ID3 is: 0.544\n",
      "best selected feature is  sepal length (cm) , its information gain is  0.5435644431995964\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    from sklearn.datasets import load_iris\n",
    "    iris = load_iris()  # dataset for classification\n",
    "    print('Datasets loaded')\n",
    "    return iris['data'], iris['feature_names'], iris['target']\n",
    "\n",
    "train_set,features,labels=load_dataset()\n",
    "model = DecisionTree('ID3')\n",
    "tree = model.ID3_create(train_set, features, labels, visible=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0' '0' '0' '0']\n",
      " ['0' '0' '0' '1']\n",
      " ['0' '1' '0' '1']\n",
      " ['0' '1' '1' '0']\n",
      " ['0' '0' '0' '0']\n",
      " ['1' '0' '0' '0']\n",
      " ['1' '0' '0' '1']\n",
      " ['1' '1' '1' '1']\n",
      " ['1' '0' '1' '2']\n",
      " ['1' '0' '1' '2']\n",
      " ['2' '0' '1' '2']\n",
      " ['2' '0' '1' '1']\n",
      " ['2' '1' '0' '1']\n",
      " ['2' '1' '0' '2']\n",
      " ['2' '0' '0' '0']\n",
      " ['2' '0' '0' '2']] ['年龄段', '有工作', '有自己的房子', '信贷情况'] ['0' '0' '1' '1' '0' '0' '0' '1' '1' '1' '1' '1' '1' '1' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "def read_dataset(filename):\n",
    "    \"\"\"\n",
    "    年龄段：0代表青年，1代表中年，2代表老年；\n",
    "    有工作：0代表否，1代表是；\n",
    "    有自己的房子：0代表否，1代表是；\n",
    "    信贷情况：0代表一般，1代表好，2代表非常好；\n",
    "    类别(是否给贷款)：0代表否，1代表是\n",
    "    \"\"\"\n",
    "    fr = open(filename,'r')\n",
    "    all_lines = fr.readlines()  ## list形式,每行为1个str\n",
    "    #print(all_lines)\n",
    "    features = ['年龄段','有工作','有自己的房子','信贷情况']\n",
    "    train_set = []\n",
    "    labels = []\n",
    "    for line in all_lines[0:]:\n",
    "        line = line.strip().split(',')  #以逗号为分割符拆分列表\n",
    "        train_set.append(line[0:-1])\n",
    "        labels.append(line[-1])\n",
    "    return np.array(train_set),features,np.array(labels)\n",
    "\n",
    "train_set,features,labels = read_dataset('./dataset.txt')\n",
    "print(train_set,features,labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}